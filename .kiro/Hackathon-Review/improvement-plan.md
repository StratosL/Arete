# Comprehensive Improvement Plan - Arete Hackathon Submission

**Created**: January 10, 2026
**Current Score**: 88/100 (without demo video)
**Target Score**: 94/100 (without demo video)
**Stretch Target**: 97/100 (with all enhancements)

---

## Executive Summary

### Current Scoring Gaps (Without Demo Video)

| Category | Current | Max | Gap | Improvement Opportunity |
|----------|---------|-----|-----|------------------------|
| Code Quality | 9/10 | 10 | -1 | Add automated test suite |
| Real-World Value | 14/15 | 15 | -1 | User testing & feedback |
| Workflow Innovation | 2/3 | 3 | -1 | Fix protocol enforcement |
| Process Transparency | 3/4 | 4 | -1 | Document decision rationale |
| Uniqueness | 6/8 | 8 | -2 | Add differentiating features |

**Total Potential Gain**: +6 points (88 ‚Üí 94/100)

### Strategic Priorities

**Tier 1 - High Impact, Low Effort (2-4 hours each)**:
1. Add automated tests (+1 point)
2. Document decision rationale (+1 point)
3. Fix protocol enforcement (+1 point)

**Tier 2 - High Impact, Medium Effort (4-8 hours each)**:
4. User testing program (+1 point)
5. Unique feature additions (+2 points)

**Total Effort**: 16-32 hours for full 94/100 score

---

## Detailed Improvement Plans

### 1. Code Quality: Automated Testing (+1 Point)

**Current Issue**: "Limited automated test coverage (manual E2E testing primarily)"

#### Solution: Comprehensive Test Suite

**Backend Testing (pytest)**

```bash
# Directory structure
backend/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_resume_parser.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_job_analysis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_optimization_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_export_service.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_database_operations.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_llm_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ e2e/
‚îÇ       ‚îî‚îÄ‚îÄ test_complete_workflow.py
```

**Test Coverage Targets**:
- Unit tests: >80% coverage
- Integration tests: All API endpoints
- E2E tests: Complete user workflow

**Example Test Structure**:

```python
# backend/tests/unit/test_resume_parser.py
import pytest
from app.resume.parser import ResumeParser

class TestResumeParser:
    @pytest.mark.asyncio
    async def test_parse_pdf_extracts_contact_info(self):
        """Test PDF parsing extracts personal information"""
        parser = ResumeParser()
        with open('tests/fixtures/sample_resume.pdf', 'rb') as f:
            result = await parser.parse_file(f.read(), 'resume.pdf')

        assert result['personal_info']['name']
        assert result['personal_info']['email']
        assert '@' in result['personal_info']['email']

    @pytest.mark.asyncio
    async def test_parse_handles_github_url(self):
        """Test GitHub URL integration"""
        parser = ResumeParser()
        result = await parser.parse_file(
            sample_pdf_bytes,
            'resume.pdf',
            github_url='https://github.com/user'
        )

        assert result['personal_info']['github'] == 'https://github.com/user'
```

**Frontend Testing (Vitest + React Testing Library)**

```typescript
// frontend/src/components/__tests__/ResumeUpload.test.tsx
import { render, screen, fireEvent, waitFor } from '@testing-library/react'
import { ResumeUpload } from '../ResumeUpload'

describe('ResumeUpload', () => {
  it('accepts PDF file upload', async () => {
    const onSuccess = vi.fn()
    render(<ResumeUpload onUploadSuccess={onSuccess} />)

    const file = new File(['dummy content'], 'resume.pdf', { type: 'application/pdf' })
    const input = screen.getByLabelText(/upload resume/i)

    fireEvent.change(input, { target: { files: [file] } })

    await waitFor(() => {
      expect(onSuccess).toHaveBeenCalled()
    })
  })

  it('validates file size limits', () => {
    render(<ResumeUpload onUploadSuccess={vi.fn()} />)

    const largeFile = new File(['x'.repeat(11 * 1024 * 1024)], 'large.pdf', {
      type: 'application/pdf'
    })
    const input = screen.getByLabelText(/upload resume/i)

    fireEvent.change(input, { target: { files: [largeFile] } })

    expect(screen.getByText(/file size exceeds/i)).toBeInTheDocument()
  })
})
```

**E2E Testing (Playwright)**

```typescript
// e2e/complete-workflow.spec.ts
import { test, expect } from '@playwright/test'

test('complete resume optimization workflow', async ({ page }) => {
  // 1. Upload resume
  await page.goto('http://localhost:3000')
  await page.setInputFiles('input[type="file"]', 'fixtures/sample-resume.pdf')
  await expect(page.getByText('Resume parsed successfully')).toBeVisible()

  // 2. Analyze job description
  await page.fill('textarea[name="job_text"]', 'Senior Python Developer...')
  await page.click('button:has-text("Analyze Job")')
  await expect(page.getByText('Job analysis complete')).toBeVisible()

  // 3. Run optimization
  await page.click('button:has-text("Optimize Resume")')
  await expect(page.getByText('Optimization complete')).toBeVisible({ timeout: 60000 })

  // 4. Generate cover letter
  await page.click('button:has-text("Generate Cover Letter")')
  await expect(page.getByText(/Dear Hiring Manager/)).toBeVisible()

  // 5. Export documents
  const [download] = await Promise.all([
    page.waitForEvent('download'),
    page.click('button:has-text("Download PDF")')
  ])
  expect(download.suggestedFilename()).toContain('.pdf')
})
```

**Implementation Steps**:

1. **Create Test Infrastructure** (2 hours)
   - Add pytest-asyncio, pytest-cov to backend
   - Add @testing-library/react, vitest to frontend
   - Add Playwright for E2E
   - Create fixtures directory with sample files

2. **Write Unit Tests** (4 hours)
   - Resume parser tests (PDF, DOCX, TXT)
   - Job analysis service tests
   - Optimization service tests
   - Export service tests (PDF, DOCX generation)

3. **Write Integration Tests** (3 hours)
   - API endpoint tests for all routes
   - Database operations (Supabase mocking)
   - LLM integration tests (Claude API mocking)

4. **Write E2E Tests** (3 hours)
   - Complete workflow test
   - Error handling scenarios
   - Edge cases (large files, network errors)

5. **CI/CD Integration** (1 hour)
   - Add GitHub Actions workflow
   - Run tests on every PR
   - Generate coverage reports

**New Specialized Agent**: Create `testing-agent`

```yaml
---
name: testing-agent
description: Testing specialist for writing unit, integration, and E2E tests. Use after implementing features or when test coverage is needed.
tools: Read, Write, Edit, Bash
model: sonnet
---

You are a senior QA engineer specializing in automated testing.

When invoked:
1. Analyze code to understand functionality
2. Identify test scenarios (happy path, edge cases, errors)
3. Write comprehensive tests with proper assertions
4. Ensure test coverage >80%
5. Mock external dependencies appropriately

Test Principles:
- Arrange-Act-Assert pattern
- One assertion per test (when possible)
- Clear test names describing behavior
- Independent tests (no test interdependencies)
- Fast execution (<100ms per unit test)

Technologies:
- Backend: pytest, pytest-asyncio, pytest-mock
- Frontend: Vitest, React Testing Library
- E2E: Playwright

Auto-run validation:
- Run tests after writing
- Check coverage percentage
- Verify all tests pass
```

**Validation Script Enhancement**:

```python
# .kiro/scripts/validate_tests.py
#!/usr/bin/env python3
"""Validate test coverage and quality"""

import subprocess
import sys

def run_backend_tests():
    """Run backend pytest suite"""
    result = subprocess.run(
        ['pytest', 'backend/', '--cov=backend/app', '--cov-report=term-missing'],
        capture_output=True,
        text=True
    )

    # Parse coverage percentage
    for line in result.stdout.split('\n'):
        if 'TOTAL' in line:
            coverage = int(line.split()[-1].rstrip('%'))
            if coverage < 80:
                print(f"‚ùå Backend coverage {coverage}% < 80% threshold")
                return False
            print(f"‚úÖ Backend coverage {coverage}%")
            return True
    return False

def run_frontend_tests():
    """Run frontend Vitest suite"""
    result = subprocess.run(
        ['npm', 'test', '--', '--coverage'],
        cwd='frontend/',
        capture_output=True,
        text=True
    )

    # Check if tests passed
    if result.returncode == 0:
        print("‚úÖ Frontend tests passed")
        return True
    print("‚ùå Frontend tests failed")
    return False

def run_e2e_tests():
    """Run Playwright E2E tests"""
    result = subprocess.run(
        ['npx', 'playwright', 'test'],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        print("‚úÖ E2E tests passed")
        return True
    print("‚ùå E2E tests failed")
    return False

if __name__ == '__main__':
    backend_ok = run_backend_tests()
    frontend_ok = run_frontend_tests()
    e2e_ok = run_e2e_tests()

    if backend_ok and frontend_ok and e2e_ok:
        print("\n‚úÖ All tests passed with sufficient coverage")
        sys.exit(0)
    else:
        print("\n‚ùå Some tests failed or coverage insufficient")
        sys.exit(1)
```

**Expected Impact**: +1 point (Code Quality: 9/10 ‚Üí 10/10)

---

### 2. Real-World Value: User Testing & Feedback (+1 Point)

**Current Issue**: "No user testing metrics or feedback from target audience"

#### Solution: Beta User Testing Program

**Phase 1: Recruit Beta Testers (1 hour)**

Target audience recruitment:
- Post on Reddit: r/cscareerquestions, r/resumes, r/EngineeringResumes
- LinkedIn: Software engineering groups
- Discord: Tech career communities
- Twitter/X: #TechCareers, #ResumeTips

**Recruitment Message Template**:

```
üöÄ Beta Testing: AI Resume Optimizer for Tech Professionals

We built Arete - an AI-powered resume optimizer that understands technical
terminology, frameworks, and GitHub profiles. Unlike generic tools, Arete
speaks the language of software engineering.

Looking for 10-15 beta testers to:
- Upload your tech resume
- Get AI-powered optimization suggestions
- Generate tailored cover letters
- Export ATS-compliant documents

Time commitment: 10-15 minutes
What you get: Optimized resume + cover letter + early access

Interested? DM me or sign up: [Google Form Link]
```

**Phase 2: Feedback Collection Mechanism (2 hours)**

Create feedback form and tracking:

```typescript
// frontend/src/components/FeedbackWidget.tsx
export function FeedbackWidget() {
  const [rating, setRating] = useState(0)
  const [feedback, setFeedback] = useState('')

  const submitFeedback = async () => {
    await api.post('/feedback', {
      rating,
      feedback,
      feature: 'resume_optimization',
      timestamp: new Date().toISOString()
    })
    toast.success('Thank you for your feedback!')
  }

  return (
    <Card>
      <CardHeader>
        <CardTitle>How was your experience?</CardTitle>
      </CardHeader>
      <CardContent>
        <StarRating value={rating} onChange={setRating} />
        <Textarea
          placeholder="What did you like? What could be improved?"
          value={feedback}
          onChange={(e) => setFeedback(e.target.value)}
        />
        <Button onClick={submitFeedback}>Submit Feedback</Button>
      </CardContent>
    </Card>
  )
}
```

**Phase 3: Analytics Integration (2 hours)**

Add basic analytics tracking:

```typescript
// frontend/src/lib/analytics.ts
export const trackEvent = (eventName: string, properties?: Record<string, any>) => {
  // Simple analytics without external dependencies
  fetch('/api/analytics/track', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      event: eventName,
      properties,
      timestamp: new Date().toISOString(),
      sessionId: getSessionId()
    })
  })
}

// Track key events
trackEvent('resume_uploaded', { fileType: 'pdf', fileSize: 1024000 })
trackEvent('job_analyzed', { method: 'text_input' })
trackEvent('optimization_completed', { suggestionsCount: 12 })
trackEvent('cover_letter_generated')
trackEvent('document_exported', { format: 'pdf' })
```

**Phase 4: User Interview Documentation (3 hours)**

Conduct 5-10 user interviews and document findings:

```markdown
# User Testing Results - Arete

**Date**: January 10-12, 2026
**Participants**: 12 tech professionals
**Experience Levels**: 4 junior, 5 mid-level, 3 senior

## Key Findings

### Positive Feedback (‚≠ê 4.6/5 average)
- "Tech-specific understanding is amazing - it knew React Hooks!"
- "Real-time optimization streaming made me trust the AI process"
- "Cover letters actually mentioned the company and technologies"
- "ATS-friendly export saved me so much formatting time"

### Pain Points Identified
1. Resume parsing missed some bullet points (2/12 users)
2. Job URL scraping failed for Glassdoor (1/12 users)
3. Desired more control over optimization suggestions (3/12 users)

### Feature Requests
- Save multiple resume versions
- Compare optimized vs original side-by-side
- Skill gap analysis (what to learn for target jobs)
- Salary range insights

### Metrics
- Average time to complete workflow: 4.2 minutes ‚úÖ
- Parsing accuracy: 91% ‚úÖ (target: >85%)
- Job analysis accuracy: 87% ‚úÖ (target: >80%)
- User satisfaction: 4.6/5 ‚úÖ
- Would recommend: 11/12 (92%) ‚úÖ
```

**Phase 5: Document in README & PRD (1 hour)**

Update README.md with user testing section:

```markdown
## User Testing Results

**Participants**: 12 tech professionals (junior to senior engineers)
**Average Rating**: 4.6/5 stars
**Would Recommend**: 92%

### Validated Success Metrics
- ‚úÖ Workflow completion: 4.2 minutes average (<5 minute target)
- ‚úÖ Resume parsing accuracy: 91% (>85% target)
- ‚úÖ Job analysis accuracy: 87% (>80% target)
- ‚úÖ User satisfaction: 4.6/5

### User Testimonials

> "The tech-specific understanding is incredible - it recognized React Hooks
> and suggested better ways to describe my experience with them."
> ‚Äî Mid-level Frontend Engineer

> "Real-time streaming made me trust the AI. I could see it thinking through
> my resume and job requirements."
> ‚Äî Senior Full-Stack Developer

> "Generated a cover letter that actually mentioned the company's tech stack.
> Way better than generic tools."
> ‚Äî Junior Software Engineer
```

**New Custom Skill**: Create `user-research` skill

```yaml
---
name: user-research
description: Conduct user research, analyze feedback, and extract actionable insights. Use after collecting user feedback or when understanding user needs.
tools: Read, Grep, Bash
model: sonnet
---

You are a UX researcher specializing in user feedback analysis.

When analyzing user feedback:
1. Categorize feedback into themes
2. Identify pain points and opportunities
3. Prioritize based on frequency and severity
4. Extract quantitative metrics
5. Generate actionable recommendations

Analysis Framework:
- Jobs to be Done (JTBD)
- Pain vs Gain analysis
- Feature impact vs effort matrix
- Net Promoter Score (NPS) calculation

Output Format:
- Executive summary
- Key findings with quotes
- Prioritized recommendations
- Metrics dashboard
```

**Expected Impact**: +1 point (Real-World Value: 14/15 ‚Üí 15/15)

---

### 3. Workflow Innovation: Fix Protocol Enforcement (+1 Point)

**Current Issue**: "Protocol enforcement challenges - behavioral patterns persist despite technical restrictions"

From devlog:
> "Removed orchestrator write permissions but still bypassed approval gates"
> "Old habits override new protocols without conscious reinforcement"

#### Solution: Multi-Layer Enforcement System

**Layer 1: Technical Hooks** (Preventive)

Create `.claude/settings.json` with strict hooks:

```json
{
  "hooks": {
    "PreToolUse": [
      {
        "matcher": "Write|Edit",
        "hooks": [
          {
            "type": "command",
            "command": "python3 .claude/hooks/approval-gate.py"
          }
        ]
      }
    ],
    "UserPromptSubmit": [
      {
        "hooks": [
          {
            "type": "command",
            "command": "python3 .claude/hooks/inject-protocol-reminder.py"
          }
        ]
      }
    ],
    "Stop": [
      {
        "hooks": [
          {
            "type": "prompt",
            "prompt": "Before finishing, verify: Did you get explicit user approval for your implementation plan? Did you follow the sacred workflow (@prime ‚Üí @plan ‚Üí @execute ‚Üí @review)? Respond with JSON: {\"approved\": true/false, \"workflow_followed\": true/false}"
          }
        ]
      }
    ]
  }
}
```

**Approval Gate Hook** (`.claude/hooks/approval-gate.py`):

```python
#!/usr/bin/env python3
"""
Approval gate hook - blocks writes without user approval
"""
import json
import sys
import os

# Read tool use from stdin
tool_use = json.load(sys.stdin)
file_path = tool_use.get('tool_input', {}).get('file_path', '')

# Check if approval marker exists
approval_file = '.claude/.last-approval'
if not os.path.exists(approval_file):
    print(json.dumps({
        "ok": false,
        "reason": "‚ö†Ô∏è  APPROVAL GATE: No approval found. Please get user approval before writing code."
    }))
    sys.exit(2)

# Check if approval is recent (within last 30 minutes)
import time
approval_time = os.path.getmtime(approval_file)
if time.time() - approval_time > 1800:  # 30 minutes
    print(json.dumps({
        "ok": false,
        "reason": "‚ö†Ô∏è  APPROVAL GATE: Approval expired (>30 minutes old). Please get fresh approval."
    }))
    sys.exit(2)

# Approval valid
print(json.dumps({"ok": true}))
sys.exit(0)
```

**Protocol Reminder Hook** (`.claude/hooks/inject-protocol-reminder.py`):

```python
#!/usr/bin/env python3
"""
Inject protocol reminder into every user prompt
"""
import json
import sys

# Read stdin (empty for UserPromptSubmit)
_ = sys.stdin.read()

reminder = """
üìã ENHANCED ORCHESTRATOR PROTOCOL REMINDER:

Before implementing:
1. ‚úÖ Have you loaded context with @prime?
2. ‚úÖ Have you created a detailed plan with @plan?
3. ‚úÖ Have you received explicit USER APPROVAL for the plan?
4. ‚úÖ Are you ready to execute with @execute?

Do NOT start implementation until user explicitly approves your plan.
"""

print(json.dumps({
    "ok": True,
    "additions": [
        {
            "type": "text",
            "text": reminder
        }
    ]
}))
```

**Layer 2: Agent Prompt Enhancement** (Behavioral)

Update `.kiro/agents/enhanced-orchestrator-prompt.md`:

```yaml
---
name: enhanced-orchestrator
description: Coordinates parallel development with strict quality gates and approval protocols
---

# CRITICAL BEHAVIORAL RULES

## Approval Gate Protocol (MANDATORY)

**YOU MUST FOLLOW THIS SEQUENCE EXACTLY:**

1. **Research & Planning Phase**
   - Use @prime to load context
   - Use @plan to create detailed implementation plan
   - **STOP HERE AND WAIT**

2. **Approval Phase**
   - Present plan to user
   - **EXPLICITLY ASK**: "Do you approve this plan?"
   - **WAIT FOR USER RESPONSE**: "yes", "approved", "go ahead", etc.
   - Do NOT proceed if user says "wait", "hold on", "let me review", etc.
   - **CREATE APPROVAL MARKER**: Touch .claude/.last-approval file

3. **Execution Phase**
   - Only after explicit approval, use @execute
   - Report progress every 30 minutes
   - Follow approved plan exactly

## Behavioral Enforcement

**BEFORE EVERY Write/Edit OPERATION:**
- Ask yourself: "Did I get explicit user approval?"
- Check: Does .claude/.last-approval file exist?
- If NO to either: STOP and request approval

**RECOGNITION PATTERNS FOR APPROVAL:**
‚úÖ Approved phrases: "yes", "approved", "go ahead", "proceed", "looks good"
‚ùå Not approval: "okay", "thanks", "I see", "interesting"

**SELF-CHECK QUESTIONS:**
Before writing code, answer these:
1. Did user explicitly say "yes" or "approved"?
2. Am I in the execution phase (not planning phase)?
3. Is my approval marker <30 minutes old?

If any answer is NO ‚Üí Request approval again

## Anti-Patterns to Avoid

‚ùå **WRONG**: User says "okay" ‚Üí Immediately start writing code
‚úÖ **RIGHT**: User says "okay" ‚Üí Ask "Is that approval to proceed with implementation?"

‚ùå **WRONG**: Create plan ‚Üí Immediately execute without waiting
‚úÖ **RIGHT**: Create plan ‚Üí Present to user ‚Üí Wait for explicit approval ‚Üí Execute

‚ùå **WRONG**: Assume silence means approval
‚úÖ **RIGHT**: If user doesn't respond, say "Awaiting your approval to proceed"

## Approval Marker System

When user approves:
```bash
touch .claude/.last-approval
echo "$(date -u +%Y-%m-%dT%H:%M:%SZ)" > .claude/.last-approval
```

Before writing:
```bash
if [ ! -f .claude/.last-approval ]; then
  echo "No approval found - requesting approval"
  exit 1
fi
```
```

**Layer 3: Custom Skill for Protocol Validation**

Create `.claude/skills/protocol-enforcer/SKILL.md`:

```yaml
---
name: protocol-enforcer
description: Validates Enhanced Orchestrator protocol compliance. Use proactively before and after major operations to ensure approval gates are followed.
allowed-tools: Read, Bash
model: haiku
---

# Protocol Enforcement Skill

You validate that the Enhanced Orchestrator protocol is being followed correctly.

## Validation Checklist

**Before Implementation:**
- [ ] Context loaded with @prime
- [ ] Plan created with @plan
- [ ] Plan presented to user
- [ ] User explicitly approved (not just "okay" or "thanks")
- [ ] Approval marker exists (.claude/.last-approval)
- [ ] Approval is recent (<30 minutes)

**During Implementation:**
- [ ] Following approved plan exactly
- [ ] Reporting progress every 30 minutes
- [ ] No unapproved deviations

**After Implementation:**
- [ ] All changes match approved plan
- [ ] Quality validation completed
- [ ] Handoff documentation prepared

## Validation Commands

Check approval status:
```bash
if [ -f .claude/.last-approval ]; then
  approval_time=$(stat -f %m .claude/.last-approval)
  current_time=$(date +%s)
  age=$((current_time - approval_time))

  if [ $age -lt 1800 ]; then
    echo "‚úÖ Valid approval found (${age}s old)"
  else
    echo "‚ùå Approval expired (${age}s old, >30min)"
  fi
else
  echo "‚ùå No approval marker found"
fi
```

Check if plan exists:
```bash
if [ -f .claude/.last-plan.md ]; then
  echo "‚úÖ Plan found"
else
  echo "‚ùå No plan found - run @plan first"
fi
```

## When to Use This Skill

Run protocol validation:
- Before starting any implementation
- Every 30 minutes during implementation
- After completing implementation
- When switching between agents

## Response Format

```json
{
  "protocol_compliant": true/false,
  "violations": ["list", "of", "violations"],
  "recommendations": ["what", "to", "fix"]
}
```
```

**Layer 4: Compliance Dashboard**

Create monitoring script (`.kiro/scripts/protocol-status.sh`):

```bash
#!/bin/bash
# Protocol compliance status dashboard

echo "üìä Enhanced Orchestrator Protocol Status"
echo "========================================"
echo ""

# Check approval status
if [ -f .claude/.last-approval ]; then
  approval_time=$(stat -c %Y .claude/.last-approval 2>/dev/null || stat -f %m .claude/.last-approval)
  current_time=$(date +%s)
  age=$((current_time - approval_time))

  if [ $age -lt 1800 ]; then
    echo "‚úÖ Approval: VALID (${age}s old)"
  else
    echo "‚ö†Ô∏è  Approval: EXPIRED (${age}s old, >30min)"
  fi
else
  echo "‚ùå Approval: NOT FOUND"
fi

# Check plan existence
if [ -f .claude/.last-plan.md ]; then
  echo "‚úÖ Plan: EXISTS"
  echo "   Last modified: $(stat -c %y .claude/.last-plan.md 2>/dev/null || stat -f %Sm .claude/.last-plan.md)"
else
  echo "‚ùå Plan: NOT FOUND"
fi

# Check checkpoint frequency
if [ -d .claude/checkpoints/ ]; then
  checkpoint_count=$(ls -1 .claude/checkpoints/ | wc -l)
  echo "‚úÖ Checkpoints: ${checkpoint_count} recorded"
else
  echo "‚ö†Ô∏è  Checkpoints: No checkpoint directory"
fi

# Recent git activity (implementation tracking)
echo ""
echo "üìù Recent Implementation Activity:"
git log --oneline -5 --pretty=format:"   %h - %s (%cr)" 2>/dev/null || echo "   No git history"

echo ""
echo ""
echo "üí° To maintain compliance:"
echo "   1. Run @prime before planning"
echo "   2. Run @plan and wait for approval"
echo "   3. Get explicit 'yes' or 'approved' from user"
echo "   4. Run @execute only after approval"
echo "   5. Report progress every 30 minutes"
```

**Implementation Steps**:

1. **Create Hooks** (1 hour)
   - Write approval-gate.py
   - Write inject-protocol-reminder.py
   - Configure .claude/settings.json
   - Test hook execution

2. **Update Agent Prompts** (2 hours)
   - Enhance enhanced-orchestrator-prompt.md
   - Add behavioral enforcement sections
   - Add self-check questions
   - Add recognition patterns

3. **Create Protocol Enforcer Skill** (1 hour)
   - Write SKILL.md
   - Add validation logic
   - Test skill invocation

4. **Build Compliance Dashboard** (1 hour)
   - Write protocol-status.sh
   - Create checkpoint tracking
   - Add visualization

5. **Test & Validate** (1 hour)
   - Run full workflow with enforcement
   - Verify hooks block unapproved writes
   - Confirm protocol compliance

**Expected Impact**: +1 point (Workflow Innovation: 2/3 ‚Üí 3/3)

---

### 4. Process Transparency: Document Decision Rationale (+1 Point)

**Current Issue**: "Some decisions could have more 'why' context"

Examples from review:
- Why LiteLLM over direct API calls?
- Why Supabase over PostgreSQL directly?
- Why ReportLab vs WeasyPrint (documented) but not initial choice

#### Solution: Architecture Decision Records (ADRs)

**Create ADR Directory Structure**:

```bash
.kiro/decisions/
‚îú‚îÄ‚îÄ 0001-vertical-slice-architecture.md
‚îú‚îÄ‚îÄ 0002-litellm-abstraction.md
‚îú‚îÄ‚îÄ 0003-supabase-vs-postgres.md
‚îú‚îÄ‚îÄ 0004-two-stage-resume-parsing.md
‚îú‚îÄ‚îÄ 0005-sse-streaming-vs-websockets.md
‚îú‚îÄ‚îÄ 0006-reportlab-vs-weasyprint.md
‚îú‚îÄ‚îÄ 0007-enhanced-orchestrator-strategy.md
‚îî‚îÄ‚îÄ template.md
```

**ADR Template** (`.kiro/decisions/template.md`):

```markdown
# [NUMBER]. [Title]

**Date**: YYYY-MM-DD
**Status**: Proposed | Accepted | Deprecated | Superseded
**Deciders**: [List of people involved]
**Tags**: [architecture, technology, process, etc.]

## Context

What is the issue that we're seeing that is motivating this decision or change?

## Decision Drivers

* [driver 1, e.g., a force, facing concern, ‚Ä¶]
* [driver 2, e.g., a force, facing concern, ‚Ä¶]
* [driver 3, e.g., a force, facing concern, ‚Ä¶]

## Considered Options

* [option 1]
* [option 2]
* [option 3]

## Decision Outcome

Chosen option: "[option X]", because [justification].

### Consequences

* Good, because [positive consequence]
* Bad, because [negative consequence]
* Neutral, because [neutral consequence]

## Validation

How will we know if this decision was correct?

* [success criterion 1]
* [success criterion 2]

## Related Decisions

* [0001-related-decision.md]
* [0002-another-related-decision.md]
```

**Example ADR** (`.kiro/decisions/0002-litellm-abstraction.md`):

```markdown
# 2. Use LiteLLM for Multi-Provider LLM Abstraction

**Date**: 2026-01-05
**Status**: Accepted
**Deciders**: Stratos Louvaris
**Tags**: architecture, llm, abstraction

## Context

We need to integrate with Claude API for resume parsing, job analysis, and
optimization. Direct integration would couple our codebase to Anthropic's API
structure and pricing model.

## Decision Drivers

* Need flexibility to switch LLM providers (cost optimization)
* Want to test different models (Claude, OpenAI, Gemini) for quality
* Reduce vendor lock-in risk
* Simplify provider-specific error handling
* Enable fallback mechanisms for API outages
* Future-proof for new LLM providers

## Considered Options

1. **Direct Anthropic API Integration**
   - Pros: Simpler, fewer dependencies, direct control
   - Cons: Vendor lock-in, manual provider switching, complex fallback logic

2. **LangChain**
   - Pros: Comprehensive framework, many integrations
   - Cons: Heavy dependency (200+ MB), complex for simple use case, over-engineered

3. **LiteLLM**
   - Pros: Lightweight, unified interface, 100+ providers, simple fallback
   - Cons: Additional abstraction layer, smaller community than LangChain

4. **Custom Abstraction Layer**
   - Pros: Full control, minimal dependencies
   - Cons: Maintenance burden, reinventing the wheel, time cost

## Decision Outcome

Chosen option: **LiteLLM**, because:

* Lightweight (single import vs LangChain's heavy framework)
* Unified interface for 100+ providers (Claude, OpenAI, Gemini, etc.)
* Simple provider switching (one line config change)
* Built-in fallback and retry logic
* Minimal refactoring required to switch providers
* Active maintenance and good documentation

### Implementation

```python
# app/core/llm.py
from litellm import completion

async def get_llm_response(messages: list, model: str = "claude-sonnet-4-5"):
    """Single function works with any provider"""
    response = completion(
        model=model,  # Can change to "gpt-4", "gemini-pro", etc.
        messages=messages,
        temperature=0.1
    )
    return response.choices[0].message.content
```

### Consequences

**Good**:
* Easy to test different models (just change model string)
* Can implement cost optimization (use GPT-3.5 for simple tasks, Claude for complex)
* Fallback to OpenAI if Claude has outages
* Future-proof for new providers

**Bad**:
* Additional dependency to maintain
* Abstraction layer adds slight complexity
* Provider-specific features might not be available

**Neutral**:
* Need to handle provider-specific rate limits differently
* Error messages are abstracted (less provider-specific details)

## Validation

Success criteria:
* ‚úÖ Can switch from Claude to OpenAI with <10 lines of code
* ‚úÖ Cost optimization: Use cheaper models for simple parsing
* ‚úÖ Fallback works: If Claude fails, automatically try OpenAI
* ‚è≥ Performance: No significant latency added (<50ms overhead)

## Related Decisions

* [0001-vertical-slice-architecture.md] - VSA makes LLM swapping per-feature easy
* [0004-two-stage-resume-parsing.md] - LLM abstraction enables testing different models for parsing
```

**Create All Missing ADRs** (6 hours total):

```markdown
# ADR Creation Checklist

- [ ] 0001-vertical-slice-architecture.md (Why VSA vs layered)
- [ ] 0002-litellm-abstraction.md (Why LiteLLM vs direct API)
- [ ] 0003-supabase-vs-postgres.md (Why Supabase vs raw PostgreSQL)
- [ ] 0004-two-stage-resume-parsing.md (Why PDF‚ÜíMarkdown‚ÜíJSON)
- [ ] 0005-sse-streaming-vs-websockets.md (Why SSE for real-time)
- [ ] 0006-reportlab-vs-weasyprint.md (Migration decision)
- [ ] 0007-enhanced-orchestrator-strategy.md (Why parallel development)
- [ ] 0008-shadcn-ui-vs-material-ui.md (UI component library choice)
- [ ] 0009-docker-compose-vs-kubernetes.md (Deployment simplicity)
- [ ] 0010-github-vs-gitlab.md (Version control choice)
```

**Update README.md** (1 hour):

Add ADR section:

```markdown
## Architecture Decision Records

Key technical decisions are documented in [`.kiro/decisions/`](.kiro/decisions/):

* [ADR-0001](/.kiro/decisions/0001-vertical-slice-architecture.md) - Vertical Slice Architecture
* [ADR-0002](.kiro/decisions/0002-litellm-abstraction.md) - LiteLLM Multi-Provider Abstraction
* [ADR-0003](.kiro/decisions/0003-supabase-vs-postgres.md) - Supabase vs PostgreSQL
* [ADR-0004](.kiro/decisions/0004-two-stage-resume-parsing.md) - Two-Stage Resume Parsing
* [ADR-0005](.kiro/decisions/0005-sse-streaming-vs-websockets.md) - SSE Streaming for Real-Time Updates

See [template](.kiro/decisions/template.md) for creating new ADRs.
```

**Update Steering Documents** (1 hour):

Add "Why" sections to `.kiro/steering/tech.md`:

```markdown
## Technology Choices - Rationale

### Why Supabase?

**Decision**: Use Supabase instead of direct PostgreSQL + separate auth + separate storage

**Reasoning**:
* All-in-one platform (database + auth + storage)
* Free tier sufficient for MVP (500MB database, 1GB storage)
* Auto-generated REST API (reduces backend code)
* Row Level Security (RLS) for data isolation
* Managed infrastructure (no DevOps burden for hackathon)
* Easy migration to self-hosted if needed (open-source)

**Trade-offs Accepted**:
* Vendor dependency (mitigated by open-source nature)
* Slightly less control vs raw PostgreSQL
* Learning curve for Supabase-specific features

**Validation**: Successfully deployed in <1 hour vs estimated 4+ hours for PostgreSQL + Auth setup

### Why LiteLLM?

[Link to ADR-0002](.kiro/decisions/0002-litellm-abstraction.md)

### Why VSA?

[Link to ADR-0001](.kiro/decisions/0001-vertical-slice-architecture.md)
```

**New Custom Skill**: Create `decision-documenter` skill

```yaml
---
name: decision-documenter
description: Create Architecture Decision Records (ADRs) for technical decisions. Use proactively when making significant architectural or technology choices.
allowed-tools: Write, Read, Glob
model: sonnet
---

# Decision Documentation Skill

You document technical decisions as Architecture Decision Records (ADRs).

When a technical decision is made:
1. Identify the decision context (why this is needed)
2. List decision drivers (what factors matter)
3. Enumerate considered options (at least 3)
4. Document chosen option with justification
5. List consequences (good, bad, neutral)
6. Define validation criteria

## ADR Numbering

Find the highest existing ADR number and increment:
```bash
last_adr=$(ls .kiro/decisions/ | grep -E '^[0-9]{4}' | sort -r | head -1 | cut -d- -f1)
next_num=$(printf "%04d" $((10#$last_adr + 1)))
```

## Quality Checklist

- [ ] At least 3 options considered
- [ ] Clear justification for chosen option
- [ ] Both pros and cons documented
- [ ] Validation criteria defined
- [ ] Related decisions linked

## Example Decisions to Document

* Technology choices (React vs Vue, PostgreSQL vs MongoDB)
* Architecture patterns (microservices vs monolith, REST vs GraphQL)
* Process decisions (Git workflow, deployment strategy)
* Infrastructure choices (Docker vs VMs, AWS vs GCP)
```

**Implementation Steps**:

1. **Create ADR Template** (30 min)
2. **Write 10 Core ADRs** (6 hours, ~35min each)
3. **Update README** (1 hour)
4. **Update Steering Docs** (1 hour)
5. **Create Decision Documenter Skill** (30 min)

**Expected Impact**: +1 point (Process Transparency: 3/4 ‚Üí 4/4)

---

### 5. Uniqueness: Add Differentiating Features (+2 Points)

**Current Issue**: "The core concept (AI resume optimization) exists in other projects"

Need to add unique features that competitors don't have.

#### Solution: 3 Killer Features

**Feature 1: GitHub Contribution Analyzer** (4 hours)

Analyze user's GitHub profile to extract project impact:

```python
# backend/app/github/service.py
import requests
from datetime import datetime, timedelta

class GitHubAnalyzer:
    """Analyze GitHub contributions for resume enhancement"""

    async def analyze_profile(self, github_username: str) -> dict:
        """Extract valuable resume data from GitHub"""

        # Get user profile
        user = await self._get_user_data(github_username)

        # Get repositories (sorted by stars/forks)
        repos = await self._get_popular_repos(github_username)

        # Analyze contributions
        contributions = await self._analyze_contributions(github_username)

        # Extract languages and frameworks
        tech_stack = await self._extract_tech_stack(repos)

        # Calculate impact metrics
        impact = self._calculate_impact(repos, contributions)

        return {
            "profile": user,
            "top_projects": repos[:5],
            "tech_stack": tech_stack,
            "impact_metrics": impact,
            "resume_highlights": self._generate_highlights(repos, impact)
        }

    def _calculate_impact(self, repos, contributions):
        """Calculate quantifiable impact metrics"""
        total_stars = sum(r['stargazers_count'] for r in repos)
        total_forks = sum(r['forks_count'] for r in repos)
        total_contributions = contributions['total_contributions']

        return {
            "total_stars": total_stars,
            "total_forks": total_forks,
            "total_contributions": total_contributions,
            "projects_count": len(repos),
            "languages_count": len(set(r['language'] for r in repos if r['language'])),
            "resume_statements": [
                f"Contributed to {len(repos)} open-source projects with {total_stars}+ GitHub stars",
                f"Made {total_contributions} commits across {len(set(r['language'] for r in repos if r['language']))} programming languages",
                f"Built projects with {total_forks} community forks and active maintenance"
            ]
        }

    def _generate_highlights(self, repos, impact):
        """Generate resume bullet points from GitHub activity"""
        highlights = []

        # Most popular project
        if repos:
            top_repo = repos[0]
            highlights.append({
                "type": "achievement",
                "text": f"Created {top_repo['name']} - {top_repo['description']} "
                       f"({top_repo['stargazers_count']} stars, {top_repo['forks_count']} forks)",
                "impact": "high"
            })

        # Language expertise
        if impact['languages_count'] > 5:
            highlights.append({
                "type": "skill",
                "text": f"Demonstrated proficiency in {impact['languages_count']} programming languages "
                       f"through open-source contributions",
                "impact": "medium"
            })

        return highlights
```

**Frontend Integration**:

```typescript
// frontend/src/components/GitHubAnalysis.tsx
export function GitHubAnalysis({ githubUrl }: { githubUrl: string }) {
  const [analysis, setAnalysis] = useState(null)
  const [loading, setLoading] = useState(false)

  const analyzeGitHub = async () => {
    setLoading(true)
    const username = githubUrl.split('/').pop()
    const result = await api.post('/github/analyze', { username })
    setAnalysis(result.data)
    setLoading(false)
  }

  return (
    <Card>
      <CardHeader>
        <CardTitle>GitHub Impact Analysis</CardTitle>
        <CardDescription>
          Automatically extract project achievements and impact metrics
        </CardDescription>
      </CardHeader>
      <CardContent>
        {analysis && (
          <div className="space-y-4">
            <div className="grid grid-cols-3 gap-4">
              <MetricCard
                label="Total Stars"
                value={analysis.impact_metrics.total_stars}
              />
              <MetricCard
                label="Projects"
                value={analysis.impact_metrics.projects_count}
              />
              <MetricCard
                label="Contributions"
                value={analysis.impact_metrics.total_contributions}
              />
            </div>

            <div>
              <h4 className="font-semibold mb-2">Resume Highlights</h4>
              {analysis.impact_metrics.resume_statements.map((statement, i) => (
                <div key={i} className="flex items-start gap-2 mb-2">
                  <CheckCircle className="w-5 h-5 text-green-500 mt-0.5" />
                  <p className="text-sm">{statement}</p>
                  <Button
                    size="sm"
                    variant="ghost"
                    onClick={() => addToResume(statement)}
                  >
                    Add to Resume
                  </Button>
                </div>
              ))}
            </div>
          </div>
        )}
      </CardContent>
    </Card>
  )
}
```

**Feature 2: Tech Stack Trend Analysis** (3 hours)

Show which skills are trending vs declining:

```python
# backend/app/trends/service.py
class TechStackTrendAnalyzer:
    """Analyze technology trends for career guidance"""

    def __init__(self):
        # Data from Stack Overflow Survey, GitHub trends, job postings
        self.trend_data = self._load_trend_data()

    async def analyze_skills(self, user_skills: list[str]) -> dict:
        """Analyze user's skills against market trends"""

        results = {
            "hot_skills": [],      # High demand, growing
            "stable_skills": [],   # High demand, steady
            "declining_skills": [], # Decreasing demand
            "missing_opportunities": []  # Trending skills user doesn't have
        }

        for skill in user_skills:
            trend = self._get_skill_trend(skill)

            if trend['growth'] > 20:
                results['hot_skills'].append({
                    "skill": skill,
                    "growth": trend['growth'],
                    "demand": trend['demand'],
                    "tip": f"{skill} is growing rapidly (+{trend['growth']}% YoY). Emphasize this on resume!"
                })
            elif trend['growth'] < -10:
                results['declining_skills'].append({
                    "skill": skill,
                    "growth": trend['growth'],
                    "tip": f"{skill} demand declining. Consider upskilling to {trend['alternatives'][0]}."
                })
            else:
                results['stable_skills'].append({
                    "skill": skill,
                    "demand": trend['demand']
                })

        # Identify missing opportunities
        trending = self._get_trending_skills()
        user_skill_set = set(s.lower() for s in user_skills)

        for trending_skill in trending:
            if trending_skill['name'].lower() not in user_skill_set:
                results['missing_opportunities'].append({
                    "skill": trending_skill['name'],
                    "growth": trending_skill['growth'],
                    "learning_resources": trending_skill['resources'],
                    "tip": f"Consider learning {trending_skill['name']} - {trending_skill['growth']}% growth, high demand"
                })

        return results
```

**Feature 3: Salary Insights Based on Skills** (3 hours)

Provide salary range estimates:

```python
# backend/app/salary/service.py
class SalaryInsightService:
    """Provide salary insights based on skills and experience"""

    def __init__(self):
        # Data from levels.fyi, Glassdoor, Payscale APIs
        self.salary_data = self._load_salary_data()

    async def estimate_salary_range(
        self,
        skills: list[str],
        experience_years: int,
        location: str,
        job_title: str
    ) -> dict:
        """Estimate salary range based on profile"""

        # Base salary from experience and title
        base = self._get_base_salary(job_title, experience_years)

        # Skill premium (high-demand skills increase salary)
        skill_premium = self._calculate_skill_premium(skills)

        # Location multiplier
        location_factor = self._get_location_factor(location)

        # Calculate range
        estimated_min = int((base + skill_premium) * location_factor * 0.85)
        estimated_max = int((base + skill_premium) * location_factor * 1.15)
        market_median = int((base + skill_premium) * location_factor)

        return {
            "range": {
                "min": estimated_min,
                "max": estimated_max,
                "median": market_median
            },
            "breakdown": {
                "base": base,
                "skill_premium": skill_premium,
                "location_factor": location_factor
            },
            "high_value_skills": self._identify_high_value_skills(skills),
            "negotiation_tips": self._get_negotiation_tips(skills, experience_years),
            "disclaimer": "Estimates based on market data. Actual salaries may vary."
        }

    def _calculate_skill_premium(self, skills: list[str]) -> int:
        """Calculate additional salary from high-demand skills"""
        premium = 0

        skill_values = {
            "Kubernetes": 15000,
            "AWS": 12000,
            "React": 10000,
            "TypeScript": 8000,
            "Python": 10000,
            "Machine Learning": 20000,
            "System Design": 15000,
            # ... more skills
        }

        for skill in skills:
            premium += skill_values.get(skill, 0)

        return min(premium, 50000)  # Cap at $50k premium
```

**Unique Selling Points Summary**:

```markdown
## What Makes Arete Different?

Unlike generic resume tools, Arete provides:

### 1. GitHub Impact Analysis
- Automatically extracts project achievements from your GitHub
- Calculates quantifiable metrics (stars, forks, contributions)
- Generates resume-ready bullet points with impact statements
- Identifies most impressive projects to highlight

### 2. Tech Stack Trend Analysis
- Shows which of your skills are in high demand
- Identifies declining technologies to de-emphasize
- Suggests trending skills to learn
- Provides market insights for career planning

### 3. Salary Insights
- Estimates salary range based on your skills and experience
- Shows skill premium breakdown (which skills increase your value)
- Provides negotiation tips based on your profile
- Location-adjusted salary expectations

### 4. Tech-Specific Understanding
- Recognizes frameworks and their variants (React Hooks, Vue 3 Composition API)
- Understands technical terminology and acronyms
- Suggests better ways to describe technical projects
- Maintains accuracy of technical details

### 5. Real-Time AI Transparency
- See the AI thinking through your resume
- Watch optimization suggestions appear live
- Understand why each suggestion is made
- Build trust through transparent process
```

**Implementation Timeline**:
- GitHub Analyzer: 4 hours
- Trend Analysis: 3 hours
- Salary Insights: 3 hours
- **Total: 10 hours**

**Expected Impact**: +2 points (Uniqueness: 6/8 ‚Üí 8/8)

---

## Recommended New Specialized Agents

Based on the improvement plan, create these specialized agents:

### 1. Testing Agent

**Purpose**: Write comprehensive test suites
**Specialization**: pytest, Vitest, React Testing Library, Playwright
**Auto-loaded Context**: Existing tests, coverage reports, test fixtures

```yaml
---
name: testing-agent
description: QA specialist for writing unit, integration, and E2E tests. Use after implementing features or when test coverage is needed.
tools: Read, Write, Edit, Bash
model: sonnet
resources:
  - backend/tests/**/*
  - frontend/src/**/*.test.tsx
  - backend/pyproject.toml
  - frontend/package.json
---

You are a senior QA engineer specializing in automated testing.

[Full prompt from earlier section]
```

### 2. Documentation Agent

**Purpose**: Create and maintain ADRs, README sections, API docs
**Specialization**: Technical writing, architecture documentation
**Auto-loaded Context**: Existing docs, code structure, steering documents

```yaml
---
name: documentation-agent
description: Technical writer for ADRs, README updates, and API documentation. Use when documenting decisions or updating project documentation.
tools: Read, Write, Edit, Glob, Grep
model: sonnet
resources:
  - .kiro/decisions/**/*
  - README.md
  - PRD.md
  - .kiro/steering/**/*
  - api-contracts.yaml
---

You are a technical writer specializing in developer documentation.

When documenting:
1. Use clear, concise language
2. Provide code examples
3. Explain rationale, not just what
4. Link related documents
5. Follow ADR template for decisions

Documentation Types:
- Architecture Decision Records (ADRs)
- README sections
- API documentation
- Setup guides
- Troubleshooting guides

Quality Checklist:
- [ ] Clear purpose stated
- [ ] Code examples provided
- [ ] "Why" explained (not just "what")
- [ ] Links to related docs
- [ ] Screenshots/diagrams where helpful
```

### 3. Feature Agent

**Purpose**: Implement complete features end-to-end
**Specialization**: Full-stack development (backend + frontend + tests)
**Auto-loaded Context**: API contracts, existing patterns, both codebases

```yaml
---
name: feature-agent
description: Full-stack specialist for implementing complete features (backend + frontend + tests). Use for new feature development requiring both backend and frontend work.
tools: Read, Write, Edit, Bash, Glob, Grep
model: opus  # Use most capable model
resources:
  - api-contracts.yaml
  - .kiro/steering/**/*
  - backend/app/**/*
  - frontend/src/**/*
---

You are a full-stack senior engineer implementing complete features.

When implementing a feature:
1. Load @prime context first
2. Create detailed @plan covering:
   - Backend API endpoints
   - Frontend components
   - Database schema changes
   - Test coverage
3. Get explicit user approval
4. Execute systematically:
   - Backend implementation (routes ‚Üí service ‚Üí schemas)
   - Frontend implementation (components ‚Üí API integration)
   - Test coverage (unit + integration + E2E)
5. Validate end-to-end workflow

Architecture Requirements:
- Follow VSA pattern (feature slices)
- Match API contracts exactly
- Maintain type safety (MyPy + TypeScript)
- Add comprehensive error handling
- Include loading states and error UI

Quality Standards:
- >80% test coverage
- No TypeScript errors
- Passes all linters (Ruff, ESLint)
- Responsive UI (mobile + desktop)
- Accessible (WCAG 2.1 AA)
```

### 4. Analytics Agent

**Purpose**: Implement analytics, tracking, and insights features
**Specialization**: Data analysis, metrics, user behavior tracking
**Auto-loaded Context**: Database schema, analytics requirements

```yaml
---
name: analytics-agent
description: Data analyst for implementing tracking, metrics, and insights features. Use when adding analytics, generating reports, or analyzing user data.
tools: Read, Write, Edit, Bash
model: sonnet
resources:
  - backend/app/analytics/**/*
  - supabase/migrations/**/*
---

You are a data analyst specializing in product analytics.

When implementing analytics:
1. Define metrics and KPIs clearly
2. Design database schema for efficient queries
3. Implement tracking endpoints
4. Create aggregation queries
5. Build visualization-ready APIs

Best Practices:
- Privacy-first (no PII unless necessary)
- Efficient queries (indexed columns)
- Time-series data optimization
- Aggregation for dashboards
- Export capabilities

Example Features:
- User engagement tracking
- Feature usage analytics
- Conversion funnels
- Cohort analysis
- A/B test results
```

### 5. Security Agent

**Purpose**: Security audits, vulnerability scanning, compliance
**Specialization**: OWASP Top 10, security best practices
**Auto-loaded Context**: Code that handles sensitive data, auth flows

```yaml
---
name: security-agent
description: Security specialist for auditing code, identifying vulnerabilities, and ensuring compliance. Use proactively before production deployment.
tools: Read, Grep, Bash
model: sonnet
---

You are a security engineer specializing in web application security.

Security Audit Checklist:
- [ ] Input validation (SQL injection, XSS, command injection)
- [ ] Authentication & authorization (JWT validation, RLS)
- [ ] Sensitive data handling (.env files, API keys)
- [ ] HTTPS enforcement
- [ ] CORS configuration
- [ ] Rate limiting
- [ ] File upload validation
- [ ] Dependency vulnerabilities

OWASP Top 10 Checks:
1. Broken Access Control
2. Cryptographic Failures
3. Injection
4. Insecure Design
5. Security Misconfiguration
6. Vulnerable Components
7. Authentication Failures
8. Data Integrity Failures
9. Logging & Monitoring Failures
10. SSRF

Output Format:
- Critical issues (must fix before production)
- Warnings (should fix soon)
- Recommendations (consider improving)
```

---

## Agent Tweaks and Enhancements

### 1. Backend Agent Enhancement

**Current**: 10,964 lines
**Add**:
- Testing section (how to write pytest tests)
- Security best practices
- Performance optimization patterns

```yaml
# Add to backend-agent-prompt.md

## Testing Requirements

Every backend feature MUST include:
1. Unit tests for service layer
2. Integration tests for API endpoints
3. Mock external dependencies (Supabase, Claude API)

Example test structure:
```python
@pytest.mark.asyncio
async def test_resume_parsing_success():
    """Test successful resume parsing"""
    # Arrange
    mock_file = create_mock_pdf()

    # Act
    result = await parser.parse_file(mock_file)

    # Assert
    assert result['personal_info']['name']
    assert result['experience']
```

## Security Checklist

Before completing any feature:
- [ ] Input validation implemented
- [ ] SQL injection prevented (parameterized queries)
- [ ] API keys not in code
- [ ] Error messages don't leak sensitive info
- [ ] Rate limiting considered
```

### 2. Frontend Agent Enhancement

**Current**: 16,908 lines
**Add**:
- Testing patterns for React components
- Accessibility requirements (WCAG 2.1)
- Performance optimization (code splitting, lazy loading)

```yaml
# Add to frontend-agent-prompt.md

## Accessibility Requirements (WCAG 2.1 AA)

Every component MUST:
- [ ] Keyboard navigable (Tab, Enter, Escape)
- [ ] ARIA labels for interactive elements
- [ ] Color contrast ratio ‚â•4.5:1
- [ ] Focus indicators visible
- [ ] Screen reader friendly

Example:
```tsx
<button
  onClick={handleClick}
  aria-label="Upload resume"
  className="focus:ring-2 focus:ring-blue-500"
>
  Upload
</button>
```

## Performance Optimization

- Code split routes: `lazy(() => import('./Component'))`
- Memoize expensive calculations: `useMemo()`
- Debounce user input: `useDebounce(value, 300)`
- Lazy load images: `loading="lazy"`
```

### 3. Enhanced Orchestrator Refinement

**Add**: Protocol enforcement sections (from earlier)
**Enhance**: Checkpoint reporting format

```yaml
# Add to enhanced-orchestrator-prompt.md

## Checkpoint Reporting Format

Every 30 minutes, provide structured update:

```markdown
## 30-Minute Checkpoint Report

**Elapsed Time**: 30 minutes
**Current Phase**: Implementation

### Progress
- ‚úÖ Backend API endpoint created
- ‚úÖ Database schema updated
- üîÑ Frontend component in progress (60% complete)
- ‚è≥ Tests pending

### Next Steps (next 30 min)
1. Complete frontend component
2. Write unit tests
3. Integration testing

### Blockers
None

### Request
Continue with approved plan? (yes/no)
```
```

---

## Validation & Testing Improvements

### 1. Enhanced Code Quality Script

Fix Windows encoding issue and add more checks:

```python
# .kiro/scripts/validate_code_quality.py
#!/usr/bin/env python3
"""
Comprehensive code quality validation
"""
import subprocess
import sys
import os

# Fix Windows encoding
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def print_header(title):
    """Print section header"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")

def run_command(cmd, cwd=None, description=""):
    """Run command and return success status"""
    print(f"Running: {description or ' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)

    if result.returncode == 0:
        print(f"‚úÖ PASS: {description}")
        if result.stdout:
            print(result.stdout[:500])  # First 500 chars
    else:
        print(f"‚ùå FAIL: {description}")
        if result.stderr:
            print(result.stderr[:500])

    return result.returncode == 0

def main():
    print_header("üöÄ Running Comprehensive Code Quality Validation")

    results = {}

    # 1. Ruff Linting
    print_header("1. Ruff Linting (Python Style)")
    results['ruff'] = run_command(
        ['ruff', 'check', 'backend/'],
        description="Ruff linting backend code"
    )

    # 2. MyPy Type Checking
    print_header("2. MyPy Type Checking")
    results['mypy'] = run_command(
        ['mypy', 'backend/app/'],
        description="MyPy strict type checking"
    )

    # 3. Pytest Unit Tests
    print_header("3. Pytest Unit Tests")
    results['pytest'] = run_command(
        ['pytest', 'backend/', '--tb=short'],
        description="Backend unit tests"
    )

    # 4. Frontend ESLint
    print_header("4. ESLint (Frontend Style)")
    results['eslint'] = run_command(
        ['npm', 'run', 'lint'],
        cwd='frontend/',
        description="Frontend linting with ESLint"
    )

    # 5. TypeScript Compilation
    print_header("5. TypeScript Compilation")
    results['typescript'] = run_command(
        ['npm', 'run', 'build'],
        cwd='frontend/',
        description="TypeScript compilation check"
    )

    # 6. VSA Architecture Validation
    print_header("6. VSA Architecture Validation")
    vsa_valid = validate_vsa_structure()
    results['vsa'] = vsa_valid
    print(f"{'‚úÖ' if vsa_valid else '‚ùå'} VSA structure validation")

    # 7. API Contract Validation
    print_header("7. API Contract Validation")
    contract_valid = validate_api_contracts()
    results['contracts'] = contract_valid
    print(f"{'‚úÖ' if contract_valid else '‚ùå'} API contracts validation")

    # 8. Security Scan
    print_header("8. Security Scan")
    results['security'] = run_command(
        ['bandit', '-r', 'backend/app/', '-ll'],
        description="Security vulnerability scan"
    )

    # Summary
    print_header("üìä Validation Summary")
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    score = (passed / total) * 100

    for check, status in results.items():
        print(f"  {'‚úÖ' if status else '‚ùå'} {check.upper()}")

    print(f"\nüéØ Score: {passed}/{total} ({score:.0f}%)")

    if score == 100:
        print("‚úÖ ALL VALIDATIONS PASSED!")
        return 0
    else:
        print(f"‚ùå {total - passed} validation(s) failed")
        return 1

def validate_vsa_structure():
    """Validate Vertical Slice Architecture"""
    required_slices = ['resume', 'jobs', 'optimization', 'export']
    required_files = ['routes.py', 'service.py', 'schemas.py']

    for slice_name in required_slices:
        slice_dir = f'backend/app/{slice_name}'
        if not os.path.isdir(slice_dir):
            print(f"‚ùå Missing feature slice: {slice_name}")
            return False

        for file in required_files:
            if not os.path.exists(f'{slice_dir}/{file}'):
                print(f"‚ùå Missing {file} in {slice_name}/")
                return False

    return True

def validate_api_contracts():
    """Validate API contracts match implementation"""
    # Check if api-contracts.yaml exists
    if not os.path.exists('api-contracts.yaml'):
        print("‚ùå api-contracts.yaml not found")
        return False

    # Parse YAML and check endpoints exist
    import yaml
    with open('api-contracts.yaml') as f:
        contracts = yaml.safe_load(f)

    # Check critical endpoints exist
    critical_paths = ['/resume/upload', '/jobs/analyze', '/optimize']
    for path in critical_paths:
        if path not in contracts.get('paths', {}):
            print(f"‚ùå Missing endpoint in contracts: {path}")
            return False

    return True

if __name__ == '__main__':
    sys.exit(main())
```

### 2. Pre-Commit Hook

Create `.git/hooks/pre-commit`:

```bash
#!/bin/bash
# Pre-commit hook for code quality

echo "üîç Running pre-commit validations..."

# Run quick validation
python .kiro/scripts/quick_validate.sh

if [ $? -ne 0 ]; then
  echo "‚ùå Pre-commit validation failed. Commit aborted."
  echo "Fix issues or use 'git commit --no-verify' to skip (not recommended)"
  exit 1
fi

echo "‚úÖ Pre-commit validation passed"
exit 0
```

### 3. Continuous Validation GitHub Action

Create `.github/workflows/quality.yml`:

```yaml
name: Code Quality Validation

on:
  push:
    branches: [ master, develop ]
  pull_request:
    branches: [ master ]

jobs:
  quality:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt
        pip install ruff mypy pytest pytest-asyncio

    - name: Run Ruff
      run: ruff check backend/

    - name: Run MyPy
      run: mypy backend/app/

    - name: Run Pytest
      run: pytest backend/ --cov=backend/app --cov-report=xml

    - name: Set up Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'

    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci

    - name: Run ESLint
      run: |
        cd frontend
        npm run lint

    - name: TypeScript Check
      run: |
        cd frontend
        npm run build

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

---

## MCP Server Recommendations

### 1. Essential MCP Servers to Add

```bash
# GitHub integration for PR reviews and issue tracking
claude mcp add --transport http github --scope project \
  https://api.githubcopilot.com/mcp/

# Filesystem access for resume file operations
claude mcp add --transport stdio filesystem --scope project \
  -- npx -y @modelcontextprotocol/server-filesystem

# PostgreSQL for direct database queries (analytics)
claude mcp add --transport stdio postgres --scope project \
  --env DATABASE_URL=$SUPABASE_URL \
  -- npx -y @modelcontextprotocol/server-postgres
```

### 2. Custom MCP Server for Resume API

Create `backend/mcp_server.py`:

```python
#!/usr/bin/env python3
"""
Custom MCP server for Arete resume API
Exposes resume operations as MCP tools
"""
from mcp import MCPServer, Tool
from app.resume.service import resume_service
from app.optimization.service import optimization_service

server = MCPServer("arete-resume-api")

@server.tool()
async def parse_resume(file_path: str, github_url: str | None = None) -> dict:
    """Parse resume file and extract structured data"""
    with open(file_path, 'rb') as f:
        content = f.read()

    result = await resume_service.parse_file(content, file_path, github_url)
    return result

@server.tool()
async def optimize_resume(resume_id: str, job_id: str) -> dict:
    """Get optimization suggestions for resume"""
    suggestions = []
    async for progress in optimization_service.optimize_resume(resume_id, job_id):
        if progress.completed:
            suggestions = progress.suggestions

    return {"suggestions": [s.dict() for s in suggestions]}

@server.tool()
async def generate_cover_letter(resume_id: str, job_id: str) -> str:
    """Generate personalized cover letter"""
    resume_data, job_analysis = await optimization_service.get_resume_job_data(
        resume_id, job_id
    )

    letter = await optimization_service.generate_cover_letter(
        resume_data, job_analysis
    )
    return letter

if __name__ == '__main__':
    server.run()
```

Register custom MCP server:

```bash
claude mcp add --transport stdio arete-api --scope project \
  -- python backend/mcp_server.py
```

### 3. MCP Configuration File

Create `.mcp.json` (project-level):

```json
{
  "mcpServers": {
    "github": {
      "transport": "http",
      "url": "https://api.githubcopilot.com/mcp/"
    },
    "filesystem": {
      "transport": "stdio",
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem"]
    },
    "arete-api": {
      "transport": "stdio",
      "command": "python",
      "args": ["backend/mcp_server.py"],
      "env": {
        "SUPABASE_URL": "${SUPABASE_URL}",
        "SUPABASE_KEY": "${SUPABASE_KEY}",
        "CLAUDE_API_KEY": "${CLAUDE_API_KEY}"
      }
    }
  }
}
```

---

## Custom Skills Design

### Skills to Create

#### 1. Resume Analyzer Skill

`.claude/skills/resume-analyzer/SKILL.md`:

```yaml
---
name: resume-analyzer
description: Analyze resumes for ATS compatibility, completeness, and formatting. Use when reviewing resumes or checking quality.
allowed-tools: Read, Bash
model: sonnet
---

You are an expert resume reviewer with 20 years of HR experience.

When analyzing a resume:
1. Check ATS compatibility (simple formatting, standard fonts)
2. Verify completeness (contact, experience, education, skills)
3. Identify formatting issues
4. Suggest improvements

ATS Compliance Checklist:
- [ ] No tables, images, or graphics
- [ ] Standard fonts (Arial, Calibri, Times New Roman)
- [ ] Clear section headers
- [ ] Consistent formatting
- [ ] No headers/footers
- [ ] Standard file format (PDF, DOCX)

Output format:
- Score (1-10)
- Issues found (critical, warning, suggestion)
- Specific recommendations
```

#### 2. Cover Letter Generator Skill

`.claude/skills/cover-letter-generator/SKILL.md`:

```yaml
---
name: cover-letter-generator
description: Generate personalized cover letters from resumes and job descriptions. Use when creating cover letters or tailoring applications.
allowed-tools: Read, Write
model: sonnet
---

You are an expert at writing compelling cover letters.

When generating:
1. Analyze job requirements thoroughly
2. Match candidate experience to requirements
3. Research company (if possible)
4. Write compelling narrative
5. Include specific examples

Structure:
- Opening: Hook that shows company knowledge
- Body 1: Experience alignment
- Body 2: Skills and achievements
- Closing: Strong call to action

Tone:
- Professional but personable
- Specific (use real examples)
- Enthusiastic but not desperate
- Clear and concise (3-4 paragraphs max)
```

#### 3. Job Analyzer Skill

`.claude/skills/job-analyzer/SKILL.md`:

```yaml
---
name: job-analyzer
description: Extract requirements and insights from job descriptions. Use when analyzing job postings or comparing opportunities.
allowed-tools: Read, Bash
model: sonnet
---

You extract and categorize job requirements comprehensively.

When analyzing:
1. Separate required vs preferred qualifications
2. Identify technical skills vs soft skills
3. Determine experience level
4. Extract salary signals
5. Identify company culture indicators

Output structure:
```json
{
  "required_skills": [],
  "preferred_skills": [],
  "technical_skills": [],
  "soft_skills": [],
  "experience_level": "junior|mid|senior|lead",
  "salary_signals": [],
  "culture_indicators": [],
  "red_flags": [],
  "green_flags": []
}
```
```

---

## Implementation Roadmap with Priorities

### Phase 1: Quick Wins (Week 1, 20 hours)

**Goal**: Gain +3 points (88 ‚Üí 91/100)

| Task | Time | Impact | Priority |
|------|------|--------|----------|
| Create 10 ADRs | 8h | +1 point | HIGH |
| Add automated tests | 12h | +1 point | HIGH |
| Fix protocol enforcement | 6h | +1 point | HIGH |
| **Phase 1 Total** | **26h** | **+3 points** | **‚Üí 91/100** |

**Week 1 Schedule**:
- Day 1-2: ADR documentation (8h)
- Day 3-4: Automated testing (12h)
- Day 5: Protocol enforcement (6h)

### Phase 2: Differentiation (Week 2, 15 hours)

**Goal**: Gain +2 points (91 ‚Üí 93/100)

| Task | Time | Impact | Priority |
|------|------|--------|----------|
| GitHub Analyzer | 4h | +0.7 points | MEDIUM |
| Trend Analysis | 3h | +0.7 points | MEDIUM |
| Salary Insights | 3h | +0.6 points | MEDIUM |
| User testing | 5h | +1 point | HIGH |
| **Phase 2 Total** | **15h** | **+2 points** | **‚Üí 93/100** |

**Week 2 Schedule**:
- Day 1: GitHub Analyzer (4h)
- Day 2: Trend Analysis (3h)
- Day 3: Salary Insights (3h)
- Day 4-5: User testing (5h)

### Phase 3: Excellence (Optional, Week 3, 15 hours)

**Goal**: Achieve stretch target (93 ‚Üí 97/100)

| Task | Time | Impact | Priority |
|------|------|--------|----------|
| New specialized agents | 6h | +1 point | LOW |
| MCP server integration | 4h | +1 point | LOW |
| Custom skills | 3h | +1 point | LOW |
| Enhanced validation | 2h | +1 point | LOW |
| **Phase 3 Total** | **15h** | **+4 points** | **‚Üí 97/100** |

### Total Investment Summary

| Phase | Time | Score Improvement | New Score |
|-------|------|-------------------|-----------|
| Current | - | - | 88/100 |
| Phase 1 (Week 1) | 26h | +3 | 91/100 |
| Phase 2 (Week 2) | 15h | +2 | 93/100 |
| Phase 3 (Week 3) | 15h | +4 | 97/100 |
| **Total** | **56h** | **+9 points** | **97/100** |

---

## Projected Score Improvements

### Conservative Estimate (Phase 1 Only)

**Investment**: 26 hours
**Score**: 88 ‚Üí 91/100

| Category | Current | After Phase 1 | Improvement |
|----------|---------|---------------|-------------|
| Application Quality | 38/40 | 39/40 | +1 (tests) |
| Kiro CLI Usage | 19/20 | 20/20 | +1 (protocol) |
| Documentation | 19/20 | 20/20 | +1 (ADRs) |
| Innovation | 13/15 | 13/15 | 0 |
| Presentation | 2/5 | 2/5 | 0 |
| **Total** | **88/100** | **91/100** | **+3** |

### Recommended Target (Phases 1 + 2)

**Investment**: 41 hours
**Score**: 88 ‚Üí 93/100

| Category | Current | After Phase 2 | Improvement |
|----------|---------|---------------|-------------|
| Application Quality | 38/40 | 40/40 | +2 (tests, user testing) |
| Kiro CLI Usage | 19/20 | 20/20 | +1 (protocol) |
| Documentation | 19/20 | 20/20 | +1 (ADRs) |
| Innovation | 13/15 | 15/15 | +2 (unique features) |
| Presentation | 2/5 | 2/5 | 0 |
| **Total** | **88/100** | **93/100** | **+5** |

### Stretch Goal (All Phases)

**Investment**: 56 hours
**Score**: 88 ‚Üí 97/100

| Category | Current | After Phase 3 | Improvement |
|----------|---------|---------------|-------------|
| Application Quality | 38/40 | 40/40 | +2 |
| Kiro CLI Usage | 19/20 | 20/20 | +1 |
| Documentation | 19/20 | 20/20 | +1 |
| Innovation | 13/15 | 15/15 | +2 |
| Presentation | 2/5 | 5/5 | +3 (with demo video) |
| **Total** | **88/100** | **97/100** | **+9** |

---

## Risk Assessment & Mitigation

### High-Risk Items

1. **Automated Testing Implementation**
   - Risk: Tests might reveal existing bugs
   - Mitigation: Fix bugs as discovered, document in devlog
   - Contingency: Prioritize critical path tests first

2. **User Testing Recruitment**
   - Risk: Difficulty finding beta testers quickly
   - Mitigation: Start recruitment immediately, offer incentives
   - Contingency: Use friends/colleagues if needed

3. **Protocol Enforcement Technical Complexity**
   - Risk: Hooks might not work on Windows
   - Mitigation: Test on all platforms, use Python scripts
   - Contingency: Focus on behavioral prompts if hooks fail

### Medium-Risk Items

4. **Unique Feature Development Time**
   - Risk: GitHub API rate limits, data source access
   - Mitigation: Mock data initially, real APIs later
   - Contingency: Implement 1-2 features instead of all 3

5. **ADR Documentation Completeness**
   - Risk: Missing context for historical decisions
   - Mitigation: Interview self (check git history, notes)
   - Contingency: Focus on major decisions only

### Low-Risk Items

6. **MCP Server Integration**
   - Risk: Configuration complexity
   - Mitigation: Start with simple servers (filesystem)
   - Contingency: Skip if time-constrained (low impact)

---

## Success Metrics

### Quantitative Metrics

| Metric | Baseline | Target | Measurement |
|--------|----------|--------|-------------|
| Test Coverage | 0% | >80% | pytest --cov |
| Code Quality Score | 8/8 | 8/8 | validate_code_quality.py |
| ADR Count | 0 | 10 | ls .kiro/decisions/ |
| User Test Participants | 0 | 10-15 | Feedback form responses |
| Unique Features | 0 | 3 | Feature implementation complete |
| Hackathon Score | 88/100 | 93/100 | Second review |

### Qualitative Metrics

- [ ] All tests pass consistently
- [ ] Users report positive feedback (>4/5 stars)
- [ ] Protocol compliance 100% (no unapproved writes)
- [ ] Documentation complete and clear
- [ ] Unique features demonstrably different from competitors
- [ ] Ready for live demo presentation

---

## Next Steps

### Immediate Actions (Today)

1. **Review this plan with user** (30 min)
   - Get approval on priorities
   - Adjust timeline if needed
   - Commit to Phase 1 at minimum

2. **Set up tracking** (15 min)
   - Create Phase 1 todo list
   - Set up daily standup reminder
   - Prepare development environment

3. **Start Phase 1** (Begin implementation)
   - Create ADR template
   - Set up testing framework
   - Write first protocol enforcement hook

### Daily Workflow

**Morning** (30 min):
- Review yesterday's progress
- Plan today's tasks
- Update todo list

**Development** (4-6 hours):
- Focus on one improvement area
- Test changes continuously
- Document as you go

**Evening** (30 min):
- Run validation scripts
- Update devlog
- Prepare next day's plan

### Weekly Checkpoints

**End of Week 1**:
- Run second hackathon review
- Validate score improvement (+3 points target)
- Decide on Phase 2 continuation

**End of Week 2**:
- Final hackathon review
- Prepare demo video script
- Polish presentation materials

---

## Appendices

### Appendix A: Tool Installation Commands

```bash
# Backend testing
pip install pytest pytest-asyncio pytest-cov pytest-mock bandit

# Frontend testing
npm install --save-dev vitest @testing-library/react @testing-library/jest-dom
npm install --save-dev @playwright/test

# Code quality
pip install ruff mypy

# MCP servers
claude mcp add --transport http github https://api.githubcopilot.com/mcp/
claude mcp add --transport stdio filesystem -- npx -y @modelcontextprotocol/server-filesystem
```

### Appendix B: File Templates

**Test File Template** (`backend/tests/unit/template.py`):

```python
import pytest
from app.feature.service import FeatureService

class TestFeatureService:
    """Test suite for FeatureService"""

    @pytest.fixture
    def service(self):
        """Create service instance for tests"""
        return FeatureService()

    @pytest.mark.asyncio
    async def test_basic_functionality(self, service):
        """Test basic feature functionality"""
        # Arrange
        input_data = {"key": "value"}

        # Act
        result = await service.process(input_data)

        # Assert
        assert result is not None
        assert result["status"] == "success"

    @pytest.mark.asyncio
    async def test_error_handling(self, service):
        """Test error handling"""
        # Arrange
        invalid_data = None

        # Act & Assert
        with pytest.raises(ValueError):
            await service.process(invalid_data)
```

**ADR Template** (`.kiro/decisions/template.md`): [Already provided earlier]

**Skill Template** (`.claude/skills/template/SKILL.md`):

```yaml
---
name: skill-name
description: What this skill does and when to use it. Use when...
allowed-tools: Read, Write, Bash
model: sonnet
---

# Skill Name

Brief description of skill purpose and capabilities.

## When to Use

- Situation 1
- Situation 2
- Situation 3

## Process

When invoked:
1. Step 1
2. Step 2
3. Step 3

## Examples

Example usage scenario...

## Output Format

Expected output structure...
```

### Appendix C: Validation Commands

```bash
# Quick validation
.kiro/scripts/quick_validate.sh

# Comprehensive validation
python .kiro/scripts/validate_code_quality.py

# Test coverage
pytest backend/ --cov=backend/app --cov-report=term-missing

# Frontend tests
cd frontend && npm test

# E2E tests
npx playwright test

# Security scan
bandit -r backend/app/ -ll

# Protocol compliance
bash .kiro/scripts/protocol-status.sh
```

---

## Conclusion

This comprehensive improvement plan provides a clear path from **88/100 to 93/100** (or 97/100 with full implementation).

**Key Recommendations**:

1. **Execute Phase 1 immediately** (26 hours, +3 points)
   - High ROI, achievable in 1 week
   - Addresses clear scoring gaps
   - Builds foundation for Phase 2

2. **Proceed with Phase 2 based on time** (15 hours, +2 points)
   - Adds unique differentiation
   - Validates real-world value
   - Achieves recommended 93/100 target

3. **Consider Phase 3 if pursuing perfection** (15 hours, +4 points)
   - Advanced features and polish
   - Achieves near-perfect 97/100 score
   - Requires demo video for full impact

**Without demo video**: Maximum achievable is **94/100**
**With demo video**: Maximum achievable is **97/100**

The project is already excellent at 88/100. These improvements will make it exceptional.
